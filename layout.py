from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
from PIL import Image
import os

from reportlab.pdfgen import canvas
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
from reportlab.lib.pagesizes import portrait

# æ³¨å†Œå­—ä½“ï¼ˆä»¥San Franciscoä¸ºä¾‹ï¼Œç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
font_path = '/System/Library/Fonts/STHeiti Medium.ttc'  # è¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´è·¯å¾„
pdfmetrics.registerFont(TTFont('STHeiti', font_path))

# è®¾ç½®PDFé¡µé¢å¤§å°ä¸º800x1067ç‚¹ï¼ˆå‡è®¾72 DPIï¼Œç›´æ¥ä½¿ç”¨åƒç´ å€¼ï¼‰
page_size = (800, 1067)
# page_size = (1000, 500)
c = canvas.Canvas("output.pdf", pagesize=page_size)

# ä½¿ç”¨æ³¨å†Œçš„å­—ä½“å’Œè°ƒæ•´å¤§å°çš„é¡µé¢
c.setFont('STHeiti', 12)
c.drawString(100, 1000, 'è¿™æ˜¯ä½¿ç”¨macOSé»˜è®¤å­—ä½“çš„æ–‡æœ¬')



def create_pdf(images, texts_zh, texts_en, output='output.pdf'):
    c = canvas.Canvas(output, pagesize=page_size)
    width, height = page_size  # ä½¿ç”¨letteré¡µé¢å¤§å°

    current_dir = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰æ–‡ä»¶å¤¹è·¯å¾„

    for i, (image_name, text_zh, text_en) in enumerate(zip(images, texts_zh, texts_en)):
        # æ„å»ºå›¾ç‰‡çš„å®Œæ•´è·¯å¾„
        image_path = os.path.join(current_dir, image_name)

        # åŠ è½½å›¾ç‰‡å¹¶è°ƒæ•´å¤§å°ä»¥ç¬¦åˆ3:4æ¯”ä¾‹
        img = Image.open(image_path)
        img_w, img_h = img.size
        target_height = height * 0.9  # å›¾ç‰‡å é¡µé¢ä¸ŠåŠéƒ¨
        aspect_ratio = 3/4
        new_width = target_height * aspect_ratio
        # img = img.resize((int(new_width), int(target_height)), Image.ANTIALIAS)
        # img = img.resize((int(new_width), int(target_height)))
        img = img.resize((int(new_width), int(target_height)), Image.Resampling.LANCZOS)


        # ä¿å­˜ä¸´æ—¶å›¾ç‰‡ç”¨äºreportlab
        temp_path = f"temp_{i}.jpg"
        img.save(temp_path)

        # æ”¾ç½®å›¾ç‰‡
        c.drawImage(temp_path, (width - new_width) / 2, height - target_height, width=new_width, height=target_height)

        # ç»˜åˆ¶æ–‡æœ¬
        text_width = width / 2  # å°†é¡µé¢å®½åº¦åˆ†ä¸ºä¸¤éƒ¨åˆ†
        c.setFont('STHeiti', 12)
        # c.drawString(100, 800, 'è¿™æ˜¯ä¸­æ–‡æ–‡æœ¬')




        c.drawString(10, height * 0.1, text_zh)  # ä¸­æ–‡æ–‡æœ¬åœ¨å·¦ä¾§
        c.setFont('Helvetica', 12)

        c.drawString(text_width + 10, height * 0.1, text_en)  # è‹±æ–‡æ–‡æœ¬åœ¨å³ä¾§

        # æ·»åŠ æ–°é¡µ
        c.showPage()

        # æ¸…ç†ä¸´æ—¶å›¾ç‰‡
        os.remove(temp_path)

    c.save()


images = [
    "BN_0000_Layer-7.png",
    "BN_0001_Layer-6.png",
    "BN_0002_Layer-5.png",
    "BN_0003_Layer-4.png",
    "BN_0004_Layer-3.png",
    "BN_0005_Layer-2.png",
    "BN_0006_Layer-1.png"
]

# ç”Ÿæˆç›¸åº”æ•°é‡çš„ä¸­æ–‡æ–‡æœ¬æ•°ç»„
texts_zh = [
    "[1] ç»™å®šâ†³ ä¸€ä¸ªåŒ…å«4ä¸ªè®­ç»ƒæ ·æœ¬çš„å°æ‰¹é‡ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰3ä¸ªç‰¹å¾ã€‚",
    "[2] çº¿æ€§å±‚â†³ é€šè¿‡æƒé‡å’Œåç½®è¿›è¡Œä¹˜æ³•æ“ä½œä»¥è·å¾—æ–°çš„ç‰¹å¾ã€‚",
    "[3] ReLUâ†³ åº”ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„æ•ˆæœæ˜¯æŠ‘åˆ¶è´Ÿå€¼ã€‚åœ¨æ­¤ç»ƒä¹ ä¸­ï¼Œ-2è¢«è®¾ä¸º0ã€‚",
    "[4] æ‰¹é‡ç»Ÿè®¡â†³ è®¡ç®—è¿™ä¸ªå°æ‰¹é‡ä¸­å››ä¸ªæ ·æœ¬çš„æ€»å’Œã€å‡å€¼ã€æ–¹å·®å’Œæ ‡å‡†å·®ã€‚â†³ æ³¨æ„ï¼Œè¿™äº›ç»Ÿè®¡é‡æ˜¯é’ˆå¯¹æ¯ä¸€è¡Œï¼ˆå³æ¯ä¸ªç‰¹å¾ç»´åº¦ï¼‰è®¡ç®—çš„ã€‚",
    "[5] å‡å€¼å½’é›¶â†³ ä»æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æ¿€æ´»å€¼ä¸­å‡å»å‡å€¼ï¼ˆç»¿è‰²ï¼‰â†³ ç›®çš„æ˜¯ä½¿æ¯ä¸ªç»´åº¦ä¸­çš„4ä¸ªæ¿€æ´»å€¼çš„å¹³å‡å€¼ä¸ºé›¶",
    "[6] æ–¹å·®å½’ä¸€â†³ é™¤ä»¥æ ‡å‡†å·®ï¼ˆæ©™è‰²ï¼‰â†³ ç›®çš„æ˜¯ä½¿4ä¸ªæ¿€æ´»å€¼çš„æ–¹å·®ç­‰äºä¸€ã€‚",
    "[7] ç¼©æ”¾ä¸åç§»â†³ å°†æ¥è‡ª[6]çš„æ ‡å‡†åŒ–ç‰¹å¾ä¹˜ä»¥çº¿æ€§å˜æ¢çŸ©é˜µï¼Œå¹¶å°†ç»“æœä¼ é€’ç»™ä¸‹ä¸€å±‚â†³ ç›®çš„æ˜¯å°†æ ‡å‡†åŒ–çš„ç‰¹å¾å€¼ç¼©æ”¾å’Œåç§»è‡³æ–°çš„å‡å€¼å’Œæ–¹å·®ï¼Œè¿™äº›æ–°çš„å‡å€¼å’Œæ–¹å·®æ˜¯ç½‘ç»œå°†è¦å­¦ä¹ çš„â†³ å¯¹è§’çº¿ä¸Šçš„å…ƒç´ å’Œæœ€åä¸€åˆ—æ˜¯ç½‘ç»œå°†è¦å­¦ä¹ çš„å¯è®­ç»ƒå‚æ•°ã€‚"
]

# ç”Ÿæˆç›¸åº”æ•°é‡çš„è‹±æ–‡æ–‡æœ¬æ•°ç»„
texts_en = [
    "[1] Givenâ†³ A mini-batch of 4 training examples, each has 3 features.",
    "[2] Linear Layerâ†³ Multiply with the weights and biases to obtain new features",
    "[3] ReLUâ†³ Apply the ReLU activation function, which has the effect of suppressing negative values. In this exercise, -2 is set to 0.",
    "[4] Batch Statisticsâ†³ Compute the sum, mean, variance, and standard deviation across the four examples in this min-batch.â†³ Note that these statistics are computed for each row (i.e., each feature dimension).",
    "[5] Shift to Mean = 0â†³ Subtract the mean (green) from the activation values for each training exampleâ†³ The intended effect is for the 4 activation values in each dimension to average to zero",
    "[6] Scale to Variance = 1â†³ Divide by the standard deviation (orange)â†³ The intended effect is for the 4 activation values to have variance equal to one.",
    "[7] Scale & Shiftâ†³ Multiply the normalized features from [6] by a linear transformation matrix, and pass the results to the next layerâ†³ The intended effect is to scale and shift the normalized feature values to a new mean and variance, which are to be learned by the networkâ†³ The elements in the diagonal and the last column are trainable parameters the network will learn."
]



create_pdf(images, texts_zh, texts_en)



# å›¾ç‰‡å’Œæ–‡æœ¬åˆ—è¡¨
# images = ['img1.png', 'img2.png', 'img3.png']  # ä½ çš„å›¾ç‰‡æ–‡ä»¶ååˆ—è¡¨
# texts_zh = ['ä¸­æ–‡æ ‡é¢˜ 1', 'ä¸­æ–‡æ ‡é¢˜ 2', 'ä¸­æ–‡æ ‡é¢˜ 3']  # ä½ çš„ä¸­æ–‡æ–‡æœ¬åˆ—è¡¨
# texts_en = ['Title 1', 'Title 2', 'Title 3']  # ä½ çš„è‹±æ–‡æ–‡æœ¬åˆ—è¡¨
    
# ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶ååˆ—è¡¨
# images = [
#     'RLHF_0000_Layer 15.jpg', 'RLHF_0005_Layer 10.jpg', 'RLHF_0010_Layer 5.jpg',
#     'RLHF_0001_Layer 14.jpg', 'RLHF_0006_Layer 9.jpg', 'RLHF_0011_Layer 4.jpg',
#     'RLHF_0002_Layer 13.jpg', 'RLHF_0007_Layer 8.jpg', 'RLHF_0012_Layer 3.jpg',
#     'RLHF_0003_Layer 12.jpg', 'RLHF_0008_Layer 7.jpg', 'RLHF_0013_Layer 2.jpg',
#     'RLHF_0004_Layer 11.jpg', 'RLHF_0009_Layer 6.jpg', 'RLHF_0014_Layer 1.jpg'
# ]

# # ç”Ÿæˆç›¸åº”æ•°é‡çš„æ–‡æœ¬æ•°ç»„
# texts_zh = [f'æ–‡å­—ğŸ¦Ÿ {i}' for i in range(1, len(images) + 1)]
# texts_en= [f'Text {i}' for i in range(1, len(images) + 1)]
